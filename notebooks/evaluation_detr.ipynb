{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# DETR Model Evaluation Dashboard\n",
    "\n",
    "This notebook evaluates the fine-tuned DETR model, reports metrics using MLOps-style visuals, and showcases predictions on test images. Feel free to extend it with additional analyses as the experiment evolves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import warnings\n",
    "from collections.abc import Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    NOTEBOOK_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    NOTEBOOK_DIR = Path.cwd().resolve()\n",
    "\n",
    "PROJECT_ROOT = NOTEBOOK_DIR\n",
    "for candidate in [NOTEBOOK_DIR, *NOTEBOOK_DIR.parents]:\n",
    "    if (candidate / \"src\").exists():\n",
    "        PROJECT_ROOT = candidate\n",
    "        break\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "eval_utils = importlib.import_module(\"src.evaluation.evaluate_detr\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"for .*: copying from a non-meta parameter\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = PROJECT_ROOT / \"models\" / \"detr-finetuned\" / \"v2\"\n",
    "TEST_IMAGES_DIR = PROJECT_ROOT / \"data\" / \"images\" / \"test\"\n",
    "BATCH_SIZE = 2\n",
    "METRIC_SCORE_THRESHOLD = 0.1\n",
    "VISUAL_SCORE_THRESHOLD = 0.5\n",
    "\n",
    "print(\n",
    "    f\"Model directory: {MODEL_DIR if MODEL_DIR.exists() else 'fallback HF checkpoint'}\"\n",
    ")\n",
    "print(f\"Test images directory: {TEST_IMAGES_DIR}\")\n",
    "\n",
    "model, processor, id2label = eval_utils.load_model(\n",
    "    MODEL_DIR if MODEL_DIR.exists() else None\n",
    ")\n",
    "print(f\"Loaded {len(id2label)} classes; model is on {eval_utils.DEVICE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, predictions, coco_gt = eval_utils.run_test_evaluation(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    score_threshold=METRIC_SCORE_THRESHOLD,\n",
    ")\n",
    "\n",
    "metrics_df = (\n",
    "    pd.Series(metrics)\n",
    "    .rename(\"value\")\n",
    "    .sort_values(ascending=False)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"metric\"})\n",
    ")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_families = []\n",
    "for name in metrics_df[\"metric\"]:\n",
    "    if name.startswith(\"mAP\"):\n",
    "        metric_families.append(\"mAP\")\n",
    "    else:\n",
    "        metric_families.append(\"AR\")\n",
    "metrics_df[\"family\"] = metric_families\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=False)\n",
    "for ax, family in zip(axes, [\"mAP\", \"AR\"], strict=False):\n",
    "    subset = metrics_df[metrics_df[\"family\"] == family]\n",
    "    if subset.empty:\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"No {family} metrics available\")\n",
    "        continue\n",
    "    plot = sns.barplot(\n",
    "        data=subset,\n",
    "        y=\"metric\",\n",
    "        x=\"value\",\n",
    "        hue=\"metric\",\n",
    "        dodge=False,\n",
    "        ax=ax,\n",
    "        palette=\"viridis\",\n",
    "    )\n",
    "    if plot.legend_:\n",
    "        plot.legend_.remove()\n",
    "    ax.set_title(f\"{family}-metrics\")\n",
    "    ax.set_xlabel(\"Score\")\n",
    "    ax.set_ylabel(\"Metric\")\n",
    "    ax.set_xlim(0, 1)\n",
    "plt.suptitle(\"Test split metric overview\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "if predictions:\n",
    "    preds_df = pd.DataFrame(predictions)\n",
    "    preds_df[\"label\"] = preds_df[\"category_id\"].map(id2label)\n",
    "    preds_df[\"score\"] = preds_df[\"score\"].round(3)\n",
    "    display(preds_df.head())\n",
    "else:\n",
    "    preds_df = pd.DataFrame(\n",
    "        columns=[\"image_id\", \"category_id\", \"bbox\", \"score\", \"label\"]\n",
    "    )\n",
    "    print(\"No predictions above the selected threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "label_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not preds_df.empty:\n",
    "    label_counts = (\n",
    "        preds_df.groupby(\"label\")[\"score\"]\n",
    "        .count()\n",
    "        .rename(\"detections\")\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    display(label_counts.set_index(\"label\"))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plot = sns.barplot(\n",
    "        data=label_counts,\n",
    "        x=\"label\",\n",
    "        y=\"detections\",\n",
    "        hue=\"label\",\n",
    "        dodge=False,\n",
    "        palette=\"mako\",\n",
    "    )\n",
    "    if plot.legend_:\n",
    "        plot.legend_.remove()\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Number of detections\")\n",
    "    plt.title(\"Detections per class (test set predictions)\")\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"No detection stats to visualize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Training data coverage per class for quick sanity checks\n",
    "DATA_VERSION = MODEL_DIR.name or \"v2\"\n",
    "train_coco_path = PROJECT_ROOT / \"data\" / \"processed\" / DATA_VERSION / \"train.json\"\n",
    "print(f\"Training dataset stats from: {train_coco_path}\")\n",
    "\n",
    "train_counts_df = pd.DataFrame(columns=[\"class\", \"image_count\"])\n",
    "if not train_coco_path.exists():\n",
    "    print(\"Training annotations not available locally.\")\n",
    "else:\n",
    "    with open(train_coco_path) as f:\n",
    "        train_coco = json.load(f)\n",
    "\n",
    "    class_lookup = {cat[\"id\"]: cat[\"name\"] for cat in train_coco.get(\"categories\", [])}\n",
    "    counts = {cid: 0 for cid in class_lookup}\n",
    "\n",
    "    ann_df = pd.DataFrame(train_coco.get(\"annotations\", []))\n",
    "    if not ann_df.empty:\n",
    "        unique_counts = ann_df.groupby(\"category_id\")[\"image_id\"].nunique().to_dict()\n",
    "        counts.update(unique_counts)\n",
    "\n",
    "    if not class_lookup:\n",
    "        print(\"No categories found in training annotations.\")\n",
    "    else:\n",
    "        ordered_ids = sorted(class_lookup)\n",
    "        train_counts_df = (\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"class\": [class_lookup[cid] for cid in ordered_ids],\n",
    "                    \"image_count\": [counts[cid] for cid in ordered_ids],\n",
    "                }\n",
    "            )\n",
    "            .sort_values(\"image_count\", ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        display(train_counts_df.set_index(\"class\"))\n",
    "\n",
    "        plt.figure(figsize=(11, 4))\n",
    "        sns.barplot(\n",
    "            data=train_counts_df,\n",
    "            x=\"class\",\n",
    "            y=\"image_count\",\n",
    "            hue=\"class\",\n",
    "            dodge=False,\n",
    "            legend=False,\n",
    "            palette=\"viridis\",\n",
    "        )\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.ylabel(\"Unique training images\")\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.title(\"Training images per class\")\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visual_helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detections(\n",
    "    ax, payload: dict, color_cycle: Iterable[tuple[float, float, float]] | None = None\n",
    ") -> None:\n",
    "    \"\"\"Draw bounding boxes and labels on a Matplotlib axis.\"\"\"\n",
    "    image = Image.open(payload[\"image_path\"]).convert(\"RGB\")\n",
    "    ax.imshow(image)\n",
    "    ax.set_axis_off()\n",
    "    boxes = payload[\"boxes\"]\n",
    "    scores = payload[\"scores\"]\n",
    "    labels = payload[\"labels\"]\n",
    "    if color_cycle is None:\n",
    "        color_cycle = sns.color_palette(\"tab10\", n_colors=max(1, len(boxes)))\n",
    "    for idx, (box, score, label) in enumerate(zip(boxes, scores, labels, strict=False)):\n",
    "        x0, y0, x1, y1 = box\n",
    "        width = x1 - x0\n",
    "        height = y1 - y0\n",
    "        color = color_cycle[idx % len(color_cycle)]\n",
    "        rect = patches.Rectangle(\n",
    "            (x0, y0), width, height, linewidth=2, edgecolor=color, facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(\n",
    "            x0,\n",
    "            max(y0 - 2, 0),\n",
    "            f\"{label} ({score:.2f})\",\n",
    "            color=\"white\",\n",
    "            fontsize=10,\n",
    "            ha=\"left\",\n",
    "            va=\"bottom\",\n",
    "            bbox=dict(facecolor=color, alpha=0.6, edgecolor=\"none\", pad=2),\n",
    "        )\n",
    "    ax.set_title(payload[\"image_path\"].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualizations",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_payloads = []\n",
    "example_images = sorted(TEST_IMAGES_DIR.glob(\"*.jpg\"))[:3]\n",
    "if not example_images:\n",
    "    print(\"No test images found for visualization.\")\n",
    "else:\n",
    "    visual_payloads = eval_utils.get_visual_predictions(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        id2label=id2label,\n",
    "        image_paths=example_images,\n",
    "        score_threshold=VISUAL_SCORE_THRESHOLD,\n",
    "    )\n",
    "    cols = len(visual_payloads)\n",
    "    fig, axes = plt.subplots(1, cols, figsize=(6 * cols, 6))\n",
    "    if cols == 1:\n",
    "        axes = [axes]\n",
    "    for ax, payload in zip(axes, visual_payloads, strict=False):\n",
    "        draw_detections(ax, payload)\n",
    "    plt.suptitle(\"Sample predictions on test images\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "if example_images and visual_payloads:\n",
    "    detailed_rows = []\n",
    "    for payload in visual_payloads:\n",
    "        for box, score, label in zip(\n",
    "            payload[\"boxes\"],\n",
    "            payload[\"scores\"],\n",
    "            payload[\"labels\"],\n",
    "            strict=False,\n",
    "        ):\n",
    "            detailed_rows.append(\n",
    "                {\n",
    "                    \"image\": payload[\"image_path\"].name,\n",
    "                    \"label\": label,\n",
    "                    \"score\": round(score, 3),\n",
    "                    \"x_min\": round(box[0], 1),\n",
    "                    \"y_min\": round(box[1], 1),\n",
    "                    \"x_max\": round(box[2], 1),\n",
    "                    \"y_max\": round(box[3], 1),\n",
    "                }\n",
    "            )\n",
    "    if detailed_rows:\n",
    "        pd.DataFrame(detailed_rows)\n",
    "    else:\n",
    "        print(\"No bounding boxes above the visualization threshold.\")\n",
    "else:\n",
    "    print(\"Skipping detail table; no visualizations available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
